            # fo_state_dict = deepcopy(net.state_dict())
            # fo_params = []
            # for i,j in enumerate(net.parameters()):
            #     fo_params.append(deepcopy(j))
            
            
            # net.load_state_dict(org_state_dict)
            # # HF params calc
            # for batch_indx,(images,labels) in enumerate(self.ldr_train3):
            #     temp = deepcopy(net.state_dict())
            #     temp_params = []
            #     for i,j in enumerate(net.parameters()):
            #         temp_params.append(deepcopy(j))
                    
            #     self.ldr_train2 = DataLoader(segmentdataset(self.dataset,self.indexes),\
            #             batch_size=int(self.bs/3),shuffle=True)
            #     for batch_indx_o,(images_o,labels_o) in enumerate(self.ldr_train2):
                    
            #         # reshuffle it
            #         self.ldr_train = DataLoader(segmentdataset(self.dataset,self.indexes),\
            #                 batch_size=int(self.bs/3),shuffle=True)
            #         optimizer = SGD_PFL(net.parameters(),lr=self.lr1)
                    
            #         for batch_index_in,(images_in,labels_in) in enumerate(self.ldr_train):
            #             images_in,labels_in = images_in.to(self.device),labels_in.to(self.device)
            #             net.zero_grad()
            #             log_probs = net(images_in)
            #             loss = self.loss_func(log_probs,labels_in)
            #             # loss.retain_grad()
            #             loss.backward()
            #             optimizer.step()
            #             break
                    
            #         optim_plus = SGD_FO_PFL(net.parameters(),temp_params,\
            #                 lr=-self.lr2)#, momentum=0.5,weight_decay=1e-4)
            #         images,labels = images.to(self.device),labels.to(self.device)
            #         net.zero_grad()
            #         log_probs = net(images)
            #         loss = self.loss_func(log_probs,labels)
            #         #loss.retain_grad()
            #         loss.backward()
            #         optimizer.step()
            #         batch_loss.append(loss.item())
                
                
                
            
            # ## inner params obtain - step size - eta_1
            # # total_loss = 0
            # for batch_indx,(images,labels) in enumerate(self.ldr_train):
            #     images,labels = images.to(self.device),labels.to(self.device)
            #     net.zero_grad()
            #     # with amp.autocast(): # not in pytorch 1.5??
            #     log_probs = net(images)
            #     loss = self.loss_func(log_probs,labels)
            #     # total_loss += loss.item()
            #     # loss.retain_grad()
                
            #     loss.backward()
            #     optimizer.step()
            #     # batch_loss.append(loss.item())
            #     # scaler.scale(loss).backward() #this computes the gradient
            #     # scaler.step(optimizer)
            # # print('loss testing')
            # # print(total_loss)
            
            # # this produces the intermediate parameters - needed inner for all three terms
            # temp_w_inner = deepcopy(net.state_dict()) #used to find intermediate loss
            # # print('w inner result')
            # # print(temp_w_inner['fc2.bias'])
            
            # temp_w_inner_params = []
            # for i,j in enumerate(net.parameters()):
            #     temp_w_inner_params.append(deepcopy(j))
            
            # ## calculate term 1 - the optim2 term on batch 2
            # # we use the same optimizer as FO_PFL for the isolated batch 2 term
            # # base params (i.e. deepcopy(temp_params)) not being updated...
            # temp_optim2_params = deepcopy(temp_params)
            # optimizer2 = SGD_FO_PFL(net.parameters(),temp_optim2_params,\
            #             lr=self.lr2)#, momentum=0.5,weight_decay=1e-4)
            # # optimizer2 = SGD_PFL(net.parameters(),lr=self.lr2)
            # print(temp_optim2_params[-1])
            # # lr = self.lr2/self.bs
            # # are the parameters updating correctly?
            
            # # total_loss = 0
            # for batch_indx,(images,labels) in enumerate(self.ldr_train2):
            #     images,labels = images.to(self.device),labels.to(self.device)
            #     net.zero_grad()
                
            #     # with amp.autocast():
            #     log_probs = net(images)
            #     loss = self.loss_func(log_probs,labels)
            #     # total_loss += loss.item()
            #     loss.retain_grad()
                
            #     loss.backward() #this computes the gradient
            #     optimizer2.step()
                
            #     print(temp_optim2_params[-1])
            #     batch_loss.append(loss.item())
            #     # scaler.scale(loss).backward()
            #     # scaler.step(optimizer2)
                
            # # print('loss testing optim2')
            # # print(total_loss)
            
            # manual_w1 = deepcopy(net.state_dict()) #first of three manual add terms
            # # print('optim2 params check')
            # # print(manual_w1['fc2.bias'])
            
            # manual_params1 = []
            # for i,j in enumerate(net.parameters()):
            #     manual_params1.append(deepcopy(j))
            
            # net.load_state_dict(temp_w_inner)
            # print(temp_w_inner['fc2.bias'])
            # print('start of optim_plus')
            ## need to check if load_state_dict also changes net.parameters()
            ### confirmed that this works as I thought
            
            # ## del_acc terms both plus and minus optim
            # # SGD optim will naturally subtract the lr
            # optim_plus = SGD_HN_PFL_del(net.parameters(),deepcopy(temp_params),\
            #                 del_acc=-self.del_acc,momentum=0.5,weight_decay=1e-4)         
            # #del_acc = -self.del_acc
            # #-self.del_acc
            
            # # optim_plus = SGD_HN_PFL_del(net.parameters(),deepcopy(temp_params),\
            #                 # del_acc=-self.del_acc,momentum=0.5,weight_decay=1e-4)       
            
            # # optim plus
            # total_loss_op = 0
            # for batch_indx,(images,labels) in enumerate(self.ldr_train2):
            #     images,labels = images.to(self.device),labels.to(self.device)
            #     net.zero_grad()
                
            #     # with amp.autocast():
            #     log_probs = net(images)
            #     loss = self.loss_func(log_probs,labels)
            #     # print(loss.item())
            #     # if loss.item() >= 100: #the grad result is so small, as the params are stable
            #         # break #need to force out otherwise the torch calc will produce nan's

            #     total_loss_op += loss.item()
            #     # print(total_loss_op)
            #     loss.retain_grad()
            
            #     loss.backward() #this computes the gradient
            #     # # print(net.state_dict()['fc2.bias'])
            #     optim_plus.step()
                
            #     # scaler.scale(loss).backward()
            #     # scaler.step(optim_plus)
                
            # # print('optim plus printing')
            # # print(net.state_dict()['fc2.bias'])
            # # print('loss_optim_plus = '+ str(total_loss_op))
            # # optim_plus_w_org = deepcopy(net.state_dict())
            
            # # cannot use torch.optim.SGD because this grad updates original params
            # optim_plus2 = SGD_HN_PFL_del(net.parameters(),deepcopy(temp_params),\
            #                 del_acc=-self.lr1*self.lr2/(2*self.del_acc),\
            #             momentum=0.5,weight_decay=1e-4)
            # # -self.lr1*self.lr2/(2*self.del_acc*self.bs)
            # #self.lr1*self.lr2/(2*self.del_acc)
            
            # # 1e-4/(2*1e-3) = ~1e-1 vs 1e-3 * 1e-2/(2*1e-3) = 1e-2
            # # looks like this self.lr2/2*self.del_acc works well for MNIST
            # # optim_plus2 = SGD_HN_PFL_del(net.parameters(),deepcopy(temp_params),\
            # #                 del_acc=self.lr2/(2*self.del_acc),\
            # #             momentum=0.5,weight_decay=1e-4)            
            
            # # optim_plus2 = SGD_HN_PFL_del(net.parameters(),deepcopy(temp_params),\
            # #                 del_acc=self.lr1/(2*self.del_acc),\
            # #             momentum=0.5,weight_decay=1e-4)
            
            # # total_loss_op2 = 0
            # for batch_indx,(images,labels) in enumerate(self.ldr_train3):
            #     images,labels = images.to(self.device),labels.to(self.device)
            #     net.zero_grad()
                
            #     # with amp.autocast():
            #     log_probs = net(images)
            #     loss = self.loss_func(log_probs,labels)
            #     loss.retain_grad()
            #     # total_loss_op2 += loss.item()
                
                
            #     batch_loss.append(loss.item()) #### this is superfluous
            #     loss.backward() #this computes the gradient
            #     optim_plus2.step()
                
            #     # scaler.scale(loss).backward()
            #     # scaler.step(optim_plus2)
                
            # optim_plus_w = deepcopy(net.state_dict())
            # # print('optim plus 2 params')
            # # print(optim_plus_w['fc2.bias'])
            # # print('optim plus 2 losses')
            # # print(total_loss_op2)
            
            # optim_plus_w_params = []
            # for i,j in enumerate(net.parameters()):
            #     optim_plus_w_params.append(deepcopy(j))
            
            # # optim minus
            # # reload to calc optim minus
            # net.load_state_dict(temp_w_inner)            
            # optim_minus = SGD_HN_PFL_del(net.parameters(),deepcopy(temp_params),\
            #                 del_acc=self.del_acc,momentum=0.5,weight_decay=1e-4) 
            # #self.del_acc
            
            # # optim_minus = SGD_HN_PFL_del(net.parameters(),deepcopy(temp_params),\
            # #                 del_acc=self.del_acc,momentum=0.5,weight_decay=1e-4)
            
            # for batch_indx,(images,labels) in enumerate(self.ldr_train2):
            #     images,labels = images.to(self.device),labels.to(self.device)
            #     net.zero_grad()
                
            #     # with amp.autocast():
            #     log_probs = net(images)
            #     loss = self.loss_func(log_probs,labels)
            #     # loss.retain_grad()
                
            #     loss.backward() #this computes the gradient
            #     optim_minus.step()
                
            #     # scaler.scale(loss).backward()
            #     # scaler.step(optim_minus)
                
            # # print('start of optim_minus')
            # # # net.load_state_dict(optim_plus_w_org)
            # # print(net.state_dict()['fc2.bias'])
            
            # optim_minus2 = SGD_HN_PFL_del(net.parameters(),deepcopy(temp_params),\
            #                 del_acc=self.lr1*self.lr2/(2*self.del_acc),\
            #             momentum=0.5,weight_decay=1e-4)
            # # *self.bs # on the denominator
            # # self.lr1*self.lr2/(2*self.del_acc*self.bs)
            # #self.lr1*self.lr2/(self.del_acc)
            
            # # optim_minus2 = SGD_HN_PFL_del(net.parameters(),deepcopy(temp_params),\
            # #                 del_acc=self.lr2/(2*self.del_acc),\
            # #             momentum=0.5,weight_decay=1e-4)                
            
            # # optim_minus2 = SGD_HN_PFL_del(net.parameters(),deepcopy(temp_params),\
            # #                 del_acc=self.lr1/(2*self.del_acc),\
            # #             momentum=0.5,weight_decay=1e-4)                         
            
            # for batch_indx,(images,labels) in enumerate(self.ldr_train3):
            #     images,labels = images.to(self.device),labels.to(self.device)
            #     net.zero_grad()
                
            #     # with amp.autocast():
            #     log_probs = net(images)
            #     loss = self.loss_func(log_probs,labels)
            #     loss.retain_grad()
                
            #     loss.backward() #this computes the gradient
            #     optim_minus2.step()
                
            #     # scaler.scale(loss).backward()
            #     # scaler.step(optim_minus2)
            
            # optim_minus_w = deepcopy(net.state_dict())
            # # print('optim minus2 params')
            # # print(optim_minus_w['fc2.bias'])
            
            # optim_minus_w_params = []
            # for i,j in enumerate(net.parameters()):
            #     optim_minus_w_params.append(deepcopy(j))
            
            # # scaler.update() #update scale for next iteration
            
            # # manual_w1, optim_plus_w, optim_minus_w combination
            # template_w = deepcopy(temp)
            
            # for k_i in template_w.keys():
            #     template_w[k_i] = manual_w1[k_i] + optim_plus_w[k_i] \
            #         + optim_minus_w[k_i] - 2*template_w[k_i]
            
            
            
            # # print('everything put together params')
            # # print(template_w['fc2.bias'])
            
            # net.load_state_dict(template_w)
            
            # net.load_state_dict(manual_w1)